#Swirl tutorial - 2. Regression

#1.Residuals
#Data is from John Verzani's website, http://wiener.math.csi.cuny.edu/UsingR/.

galton <- read.csv(file.path(find.package("swirl"), "Courses/Regression_Models/Introduction", "galton.csv"))
est <- function(slope, intercept)intercept + slope*galton$parent
sqe <- function(slope, intercept)sum( (est(slope, intercept)-galton$child)^2)
attenu <- datasets::attenu
fname <- paste(find.package("swirl"),"Courses/Regression_Models","Residuals","res_eqn.R",sep="/")

| This lesson will focus on the residuals, the distances between the actual children's heights and the estimates given by
| the regression line. Since all lines are characterized by two parameters, a slope and an intercept, we'll use the least
| squares criteria to provide two equations in two unknowns so we can solve for these parameters, the slope and intercept.

...

|=======                                                                                                            |   6%

| The first equation says that the "errors" in our estimates, the residuals, have mean zero. In other words, the residuals
| are "balanced" among the data points; they're just as likely to be positive as negative. The second equation says that our
| residuals must be uncorrelated with our predictors, the parents’ height. This makes sense - if the residuals and
| predictors were correlated then you could make a better prediction and reduce the distances (residuals) between the actual
| outcomes and the predictions.

...

|===========                                                                                                        |   9%

| We'll demonstrate these concepts now. First regenerate the regression line and call it fit. Use the R function lm. Recall
| that by default its first argument is a formula such as "child ~ parent" and its second is the dataset, in this case
| galton.

> fit <- lm(child ~ parent, galton)

| That's the answer I was looking for.

|==============                                                                                                     |  12%

| Now we'll examine fit to see its slope and intercept. The residuals we're interested in are stored in the 928-long vector
| fit$residuals. If you type fit$residuals you'll see a lot of numbers scroll by which isn't very useful; however if you
| type "summary(fit)" you will see a more concise display of the regression data. Do this now.

> summary(fit)

Call:
lm(formula = child ~ parent, data = galton)

Residuals:
Min      1Q  Median      3Q     Max 
-7.8050 -1.3661  0.0487  1.6339  5.9264 

Coefficients:
Estimate Std. Error t value Pr(>|t|)    
(Intercept) 23.94153    2.81088   8.517   <2e-16 ***
parent       0.64629    0.04114  15.711   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.239 on 926 degrees of freedom
Multiple R-squared:  0.2105,	Adjusted R-squared:  0.2096 
F-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16


| You are really on a roll!

|==================                                                                                                 |  16%

| First check the mean of fit$residuals to see if it's close to 0.

> mean(fit$residuals)
[1] -2.359884e-15

| You nailed it! Good job!
  
  |======================                                                                                             |  19%

| Now check the correlation between the residuals and the predictors. Type "cov(fit$residuals, galton$parent)" to see if
| it's close to 0.

> cov(fit$residuals, galton$parent)
[1] -1.790153e-13

| All that practice is paying off!

|=========================                                                                                          |  22%

| As shown algebraically in the slides, the equations for the intercept and slope are found by supposing a change is made to
| the intercept and slope. Squaring out the resulting expressions produces three summations. The first sum is the original
| term squared, before the slope and intercept were changed. The third sum totals the squared changes themselves. For
| instance, if we had changed fit’s intercept by adding 2, the third sum would be the total of 928 4’s. The middle sum is
| guaranteed to be zero precisely when the two equations (the conditions on the residuals) are satisfied.

...

|=============================                                                                                      |  25%

| We'll verify these claims now. We've defined for you two R functions, est and sqe. Both take two inputs, a slope and an
| intercept. The function est calculates a child's height (y-coordinate) using the line defined by the two parameters,
| (slope and intercept), and the parents' heights in the Galton data as x-coordinates.

...

|================================                                                                                   |  28%

| Let "mch"" represent the mean of the galton childrens' heights and "mph"" the mean of the galton parents' heights. Let
| "ic" and "slope" represent the intercept and slope of the regression line respectively. As shown in the slides and past
| lessons, the point (mph,mch) lies on the regression line. This means

1: mch = ic + slope*mph
2: mph = ic + slope*mch
3: I haven't the slightest idea.

Selection: 1

| Perseverance, that's the answer.

|====================================                                                                               |  31%

| The function sqe calculates the sum of the squared residuals, the differences between the actual children's heights and
| the estimated heights specified by the line defined by the given parameters (slope and intercept).  R provides the
| function deviance to do exactly this using a fitted model (e.g., fit) as its argument. However, we provide sqe because
| we'll use it to test regression lines different from fit.

...

|========================================                                                                           |  34%

| We'll see that when we vary or tweak the slope and intercept values of the regression line which are stored in fit$coef,
| the resulting squared residuals are approximately equal to the sum of two sums of squares - that of the original
| regression residuals and that of the tweaks themselves. More precisely, up to numerical error,

...

|===========================================                                                                        |  38%

| sqe(ols.slope+sl,ols.intercept+ic) == deviance(fit) + sum(est(sl,ic)ˆ2 )

...

|===============================================                                                                    |  41%

| Equivalently, sqe(ols.slope+sl,ols.intercept+ic) == sqe(ols.slope, ols.intercept) + sum(est(sl,ic)ˆ2 )

...

|==================================================                                                                 |  44%

| The left side of the equation represents the squared residuals of a new line, the "tweaked" regression line. The terms
| "sl" and "ic" represent the variations in the slope and intercept respectively. The right side has two terms. The first
| represents the squared residuals of the original regression line and the second is the sum of squares of the variations
| themselves.

...

|======================================================                                                             |  47%

| We'll demonstrate this now. First extract the intercept from fit$coef and put it in a variable called ols.ic . The
| intercept is the first element in the fit$coef vector, that is fit$coef[1].

> ols.ic <-fit$coef[1]

| You are quite good my friend!

|==========================================================                                                         |  50%

| Now extract the slope from fit$coef and put it in the variable ols.slope; the slope is the second element in the fit$coef
| vector, fit$coef[2].

> ols.slope <- fit$coef[2]

| You nailed it! Good job!

|=============================================================                                                      |  53%

| Now we'll show you some R code which generates the left and right sides of this equation.  Take a moment to look it over.
| We've formed two 6-long vectors of variations, one for the slope and one for the intercept. Then we have two "for" loops
| to generate the two sides of the equation.

...

|=================================================================                                                  |  56%

| Subtract the right side, the vector rhs, from the left, the vector lhs, to see the relationship between them. You should
| get a vector of very small, almost 0, numbers.

> rhs - lhs
[1] -1.264198e-09 -2.527486e-09 -3.801688e-09  1.261469e-09  2.522938e-09  3.767127e-09

| Almost! Try again. Or, type info() for more options.

| Type "lhs-rhs" at the R prompt.

> lhs - rhs
[1]  1.264198e-09  2.527486e-09  3.801688e-09 -1.261469e-09 -2.522938e-09 -3.767127e-09

| You are amazing!

|====================================================================                                               |  59%

| You could also use the R function all.equal with lhs and rhs as arguments to test for equality. Try it now.

> all.(lhs, rhs)
Erreur : impossible de trouver la fonction "all."
> all.equal(lhs, rhs)
[1] TRUE

| You got it right!

|========================================================================                                           |  62%

| Now we'll show that the variance in the children's heights is the sum of the variance in the OLS estimates and the
| variance in the OLS residuals. First use the R function var to calculate the variance in the children's heights and store
| it in the variable varChild.

> varChild <- var(galton$child)

| Keep up the great work!

|===========================================================================                                        |  66%

| Remember that we've calculated the residuals and they're stored in fit$residuals. Use the R function var to calculate the
| variance in these residuals now and store it in the variable varRes.

> varRes <- var(fit$residuals)

| That's the answer I was looking for.

|===============================================================================                                    |  69%

| Recall that the function "est" calculates the estimates (y-coordinates) of values along the regression line defined by the
| variables "ols.slope" and "ols.ic". Compute the variance in the estimates and store it in the variable varEst.

> View(est)

| Nice try, but that's not exactly what I was hoping for. Try again. Or, type info() for more options.

| Type "varEst <- var(est(ols.slope, ols.ic))" at the R prompt.

> View(est)

| Give it another try. Or, type info() for more options.

| Type "varEst <- var(est(ols.slope, ols.ic))" at the R prompt.

> varEst <- var(est(ols.slope, ols.ic))

| You nailed it! Good job!

|===================================================================================                                |  72%

| Now use the function all.equal to compare varChild and the sum of varRes and varEst.

> all.equal(varChild,sum(varRes,varEst))
[1] TRUE

| That's not exactly what I'm looking for. Try again. Or, type info() for more options.

| Type "all.equal(varChild,varEst+varRes)" at the R prompt.

> all.equal(varChild,(varRes+varEst)
+ )
[1] TRUE

| Not quite, but you're learning! Try again. Or, type info() for more options.

| Type "all.equal(varChild,varEst+varRes)" at the R prompt.

> all.equal(varChild,(varRes+varEst))
[1] TRUE

| Not quite! Try again. Or, type info() for more options.

| Type "all.equal(varChild,varEst+varRes)" at the R prompt.

> all.equal(varChild,varRes+varEst)
[1] TRUE

| Nice work!

|======================================================================================                             |  75%

| Since variances are sums of squares (and hence always positive), this equation which we've just demonstrated,
| var(data)=var(estimate)+var(residuals), shows that the variance of the estimate is ALWAYS less than the variance of the
| data.

...

|==========================================================================================                         |  78%

| Since var(data)=var(estimate)+var(residuals) and variances are always positive, the variance of residuals

1: is less than the variance of data
2: is unknown without actual data
3: is greater than the variance of data

Selection: 1

| All that practice is paying off!

|=============================================================================================                      |  81%

| The two properties of the residuals we've emphasized here can be applied to datasets which have multiple predictors. In
| this lesson we've loaded the dataset attenu which gives data for 23 earthquakes in California. Accelerations are estimated
| based on two predictors, distance and magnitude.

...

|=================================================================================================                  |  84%

| Generate the regression line for this data. Type efit <- lm(accel ~ mag+dist, attenu) at the R prompt.

> efit <- lm(accel ~ mag+dist, attenu)

| You are really on a roll!

|=====================================================================================================              |  88%

| Verify the mean of the residuals is 0.

> mean(efit$residuals)
[1] -1.785061e-18

| You are doing so well!

|========================================================================================================           |  91%

| Using the R function cov verify the residuals are uncorrelated with the magnitude predictor, attenu$mag.

> cov(efit$residuals, attenu$mag)
[1] 5.338694e-17

| Keep working like that and you'll get there!

|============================================================================================================       |  94%

| Using the R function cov verify the residuals are uncorrelated with the distance predictor, attenu$dist.

> cov(efit$residuals, attenu$dist)
[1] 5.253433e-16

| You got it right!

|===============================================================================================================    |  97%

| Congrats! You've finished the course on Residuals. We hope it hasn't left a bad taste in your mouth.
